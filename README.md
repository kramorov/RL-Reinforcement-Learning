# RL-Reinforcement-Learning
## Обучение с подкреплением
### Целью написания этой статьи является простым языком рассказать об обучении с подкреплением, дать описание англоязычным терминам, используемым в RL, в том значении, как они используются в функциональном API TensorFlow.

Текст написан намеренно упрощенно, так как рассчитан и на тех, кто недостаточно хорошо знаком с математикой и/или Python/TensorFlow.

Главными действующими лицами в RL являются агент (agent) и среда (environment). Среда - это мир, где обитает агент и с которой он взаимодействует. Взаимодействие осуществляется пошагово. На каждом шаге агент видит наблюдение (observation) или состояние (state) среды, и принимает решение, какое действие (action) предпринять. Выполняя наблюдение (observation), агент не всегда может видеть состояние среды или среду целиком. 

Скажем, в карточной игре, агент не может видеть карты соперника или карты в колоде. Если использоовать эту аналогию, то средой (environment) будут карты на руках у игроков и в колоде, а наблюдением (observation) агента - только карты, которые вскрыты. Продолжая аналогию карточной игры, шаг игры состоит из ходов игроков. После каждого хода меняется состояние (state) среды, и меняется наблюдение игрока. Таким образом, среда, состояние среды может меняться как под воздействием игрока, так и без его участия.

Агент получает сигнал от среды в виде вознаграждения или награды (reward). Русский перевод этого термина не точно воспроизводит смысл этого термина. Reward в терминах RL может быть как наградой (поощрением), так и наказанием (депремированием). В дальнейшем на русском я буду использовать слово "награда". 

Награда представляет собой некоторое число, которое показывает результат действий агента. Награда выдается средой не на каждом шаге, а только при каком-то изменении среды. Например, в пиг-понге, футболе и т.п. это гол. В карточной игре - завершение очередного розыгрыша. В гонках - столкновение с препятствием или поднятие бонуса (канистра, шина и т.п.).

Целью агента является максимизировать суммарное количество наград за какой-то промежуток времени (сет в теннисе, партия в шашках, продержаться 50 секунд в бою и т.п.). Суммарное количество наград назовем "результат" (return).

Целью обучения с подкреплением является обучение агента такому поведению, чтобы результат был максимальный.

Сначала обсудим термины, используемые в RL:
* Состояния  и наблюдения (states and observations);
* Пространство действий (action space);
* Политики (policies);
* Траектории (trajectories);
* Различные формулировки результата (different formulations of return);
* Функции ценности (value functions).

### Состояния и наблюдения / States and Observations
Состояние (state) S является полным описанием среды. Никакая другой информации о среде не имеется. Наблюдение (observation) может не содержать в себе всю информацию о среде, то есть может являться лишь частичным описанием среды.

В глубоком обучении с подкреплением (deep RL) состояния среды или наблюдения являются численным вектром (1D), матрицей (2D), в случае большего количества измерений мы называем это тензором. Например, наблюдение в виде изображения может бють представлено в виде RGB матриц значений пикселов, состояние робота может быть описано вектором, состоящим из значений  положений его частей и/или скорости их движения.

Когда агент в наблюдении видит полное состояние среды, это называется "полностью наблюдаемой средой" (fully observed environment). Когда агент в наблюдении видитлишь какую-то часть состояния среды, это называется "частично наблюдаемой средой" (partially observed environment).

### Пространство действий / Action Space
В различных средах допускаются различные действия агентов. Набор допустимых со стороны агента действий в данной конкретной среде часто называют "пространством действий". 

В некоторых средах, таких как игры Атари, шашки, карточные игры и т.п., пространство действий является дискретным. Проще говоря, представляет собой конечный набор чисел, каждое из которых означает какое-то отдельное действие (вверх, вниз, включить, выключить и так далее.

В других средах пространство действий может быть протяженным (continuous action space). То есть, каждое действие может быть любым числом из какого-то заданного диапазона значений. Пример - органы управления роботом в реальном мире. В RL они, как правило, являются вещественными векторами (real-valued vectors). Программист бы сказал - одномерный массив чисел типа float.

Тип пространства действий напрямую влияет на выбор алгоритмов глубокого обучения с подкреплением. Некоторые семейства алгоритмов могут быть применены без глубокой переделки только к одному типу пространства действий.

### Политики / Policies
Политика (policy) это правило, используемое агентом для принятия решения, какое действие предпринять. С точки зрения агента, это число или тензор из пространства действий.

Политика может быть **детерминированной**, то есть это число или тензор, значение которых получается путем каких-то вычислений на основании состояния среды. То есть, оно не является случайным.

Политика может быть **стохастической**. Это по-прежнему число или тензор. Однако значения этого числа или тензора может быть различным в разные моменты времени при одинаковом состоянии среды.

В глубоком обучении с подкреплением используются **параметрические политики**. То есть, выход политики является результатом вычисления какой-то функции, на вход котороой подается набор параметров, например веса и биас из НС. В процессе обучения мы подбираем значения параметров с помощью некоторого алгоритма оптимизации. 

Политика является механизмом принятия решения агентом, можно сказать, его мозгом. Имеено поэтому в литературе нередко заменяют термин "политика" словом "агент". Например, "политика нацелена на максимизацию результата".

На практике, красивый термин "политика" по сути является нашей нейронной сетью.

#### Детерминированные политики / Deterministic policies
Простым примером **детерменированной политики** является модель многослойного **перцептрона** (multi-layer perceptron (**MLP**)). 
```
model = Sequential()
model.add(Dense(10, activation='relu', input_shape=(n_features,)))
model.add(Dense(8, activation='relu', ))
model.add(Dense(1, activation='tanh'))>
```
Если на вход такой сети после подать вектор длиной n_features, на выходе мы получим число в диапазоне от -1 до +1. То есть, пространсто действий для этой политики будет протяженным, диапазон значений пространства действий [-1, +1].
Эту же архитектуру можно использовать для дискретного пространства действий, просто заменив функцию активации в выходном слое на softmax:
```
model = Sequential()
model.add(Dense(10, activation='relu', input_shape=(n_features,)))
model.add(Dense(8, activation='relu', ))
model.add(Dense(1, activation='softmax'))
```
Вы этом случае, детерминированная политика, вычисляемая нашей НС будет ориаентирована на дискретное пространство действий, так как результатом вычислений будут 2 значения: 0 или 1. Этими значениями могут быть закодированы какие-то дейтсвия, например, "0" - движение вверх, "1" - движение вниз.

#### Стохастические политики / Stochastic Policies
Выходом детерминированной политики является число или тензор, обозначающих значение чего-либо. Выходом стохастической политики также является число или тензор, однако он обозначает не величину действия из пространства действий, а **ВЕРОЯТНОСТЬ** действия.

Для обучения и работы стохастической политики необходимо сделать 2 ключевых вычисления:
* получить действия из политики
* затем вычислить логарифм вероятности действий.

Наиболее часто используемыми в обучении с подкреплением разновидностями политик являются **категорийная политика** и **диагональная Гауссова политика**.
##### Категорийная политика
Категорийные политики используются при дискретном пространстве действий, а диагональные Гауссовы  - в протяженном просстранстве действий.

С точки зрения теории вероятности, категорийная политика представляет собой [распределение Бернулли](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%91%D0%B5%D1%80%D0%BD%D1%83%D0%BB%D0%BB%D0%B8). Это означает что величина может принимать только два значание - 0 и 1, значение 0 с вероятностью P, а значение 1 - вероятностью (1-P). 

В качестве примера, вернемся к нашей сети перцептрона, только в выходном слое будет не один нейрон, а несколько (в нашем примере - четыре):

model = Sequential()
model.add(Dense(10, activation='relu', input_shape=(n_features,)))
model.add(Dense(8, activation='relu', ))
model.add(Dense(4, activation='softmax'))
Применение активационной функции '**softmax**' приведет к тому, что на выходе мы получим вектор из 4 чисел, сумма этих чисел будет равна 1. То есть, они являются вероятностями отнесения входных данных в какому-то классу.

В задачах *классификации*, обычно выбирается наибольшее значение из этого вектора, чтобы определить класс. Предположим, мы имеем на выходе вектор **[0.2 0.3 0.4 0.1]**. Наибольшее значение имеется в 3 по счету (или в индексе "2" массива), что позволяет нам сказать, что входные данные относятся к классу 3 ("2").

В стохастической политике значения в каждом индексе этого вектора рассматриваются именно как вероятности. Каждый индекс выходного вектора соответствует какому-то действию, а значение под этим индексом - вероятностью выполнения этого действия. То есть, выбираемое политикой действие каждый раз (ну, почти) будет разным, в зависимости от вероятности его наступления.

##### Диагональная Гауссова политика
В теории вероятности и мат. статистике, **многомерное распределение Гаусса** является обобщением обычного нормального распределения на N-мерное пространство. Напомним, что нормальное распределение также называемое распределением Гаусса или Гаусса — Лапласа — распределение вероятностей, которое в одномерном случае задаётся функцией плотности вероятности, совпадающей с функцией Гаусса. Случайный вектор, имеющий многомерное нормальное распределение, называется гауссовским вектором.

Многомерное распределение Гаусса описывается вектором средних значений и его ковариационной матрицей.

Диагональное Гауссово распределение является особым случаем, где ковариационная матрица имеет ненулевые значения только по диагонали. С точки зрения теории вероятности, это означает, что случайные величины X_1, X_2, ..., X_n имеют одномерное нормальное распределение и совместно независимы, а случайный вектор, составленный из них, имеет многомерное нормальное распределение. Слова "совместно независимы" означают, что наступление одного события X_k не изменяет вероятность наступления другого.

#### Траектория / Trajectories
Траектория (trajectory) - последовательный список состояний среды и действий агента в соответствии с его политикой.

Хотя правильнее было бы сказать, что с точки зрения агента, траектория это список наблюдений и действий.

В литературе траектории также часто называют эпизодами (episodes) или rollouts (близкий по смыслу с т.з НС перевод - "результат испытаний").

С точки зрения TensorFlow, траектория состоит из наблюдения, действий, награды и (как опция) дисконта.
```python
tf_agents.trajectories.trajectory.from_episode(
    observation: tf_agents.typing.types.NestedSpecTensorOrArray,
    action: tf_agents.typing.types.NestedSpecTensorOrArray,
    policy_info: tf_agents.typing.types.NestedSpecTensorOrArray,
    reward: tf_agents.typing.types.NestedSpecTensorOrArray,
    discount: Optional[types.SpecTensorOrArray] = None
) -> tf_agents.trajectories.Trajectory
```
#### Награда и результат / Reward and Return
Функция расчета награды (reward function) критически важна в обучении с подкреплением. В общем случае, она зависит от состояния среды на текущем (t) шаге, действий агента на текущем шаге и состоянии (state) среды на следующем шаге (t+1). Однако зачастую, используют упрощенный вариант, когда эта функция зависит только от состояния среды на текущем шаге и действий агента на текущем шаге.

Целью обучения с подкреплением является такое поведение агента, чтобы максимизировать суммарную награду на траектории, то есть результат (напомню, это сумма наград на каждом шаге траектории). Результат можно определять несколькими способами, в зависимости от того, чего мы хотим добиться от обучения.

**Вариант 1.**

Результат определяется как сумма наград, полученных за заранее определенное ограниченное число шагов ("ограниченный временным горизонтом недисконтированный результат" - "finite-horizon undiscounted return".

**Вариант 2.**
Результат также определяется как сумма наград, полученных за все время (игры, например), однако награда на каждом шагу (за каждый эпизод) дисконтируется на величину gamma, называемой дисконт-фактором. То есть, расчетная величина награды на текущем шаге зависит от того, как далеко в будуещем она будет действительно получена. Иными словами, фактически награда за будет получена от среды на каком-то шаге в будущем. На текущем шаге и на промежуточных шагах наград от среды не будет. Однако для стимулирования обучения в направлении получения фактической награды в будущем, функция расчета награды (reward function) каждому шагу на пути к награде приписывает какое-то фиктивное (в смысле фактически пока не полученное от среды значение награды). Это значение постепенно растет с каждым шагом с приближением к шагу с получением награды. Скорость роста, а в общем случае - алгоритм расчета промежуточных наград определяется функцией расчета награды.

Или можно сформулировать наоборот: если в какой-то момент времени TT (т.е. шаг в траектории номер TT) в будущем (TT больше номера текущего шага в траектории) от среды будет получена награда в размере R, то на каждом шаге в прошлое от момента получения награды, т.е. в моменты времени {TT-1, TT-2, ..., сейчас} функцией расчета награды быдет расчитана промежуточная награда - последовательно уменьшающаяся в размере относительно фактически полученной награды R. Как уже говорилось, функция расчета награды делает вычисления размера промежеточной награды в зависимости от параметра gamma и расстояния в шагах от момента фактического получения награды. Практическая реализация алгоритма функции может быть произвольной, в зависимости от характера задачи, которую мы решаем.

Нужно заметить, что термины "**дисконтирование**" и "**дисконт-фактор**" пришли из экономической теории. В экономической теории считается, что стоимость денег "завтра" не равна стоимости денег "сегодня". То есть, если положить деньги в банк под процент, то через какое-то время на положенную в банк сумму будут начислены проценты, и количество денег на счете в банке увеличится. Пусть их стало 120% относительно положенной на счет в банке суммы. Тогда, если считать за 100% количество денег в банке после получения процентов за 100%, то сегодня мы положим в банк лишь 100/120 = 83% от суммы, которая будет в банке в будущем.

Функция расчета наград не только награждает, но и депремирует. То есть, если когда-то в будущем будет получена не награда, а штраф (отрицательная награда), функция должна так же рассчитать промежуточные награды (в этом случае - отрицательной награды) для каждого промежуточного шага, дисконтировав полученный штраф в соответствии с расстоянием в шагах до момента получения штрафа.

#### Функция ценности / Value function
Функция ценности - это ожидаемый результат(expected return) то есть, сумма наград и депремирований, начиная с какогото шага или пары (состояние, действие) на шаге s и до какого-то момента в будущем. Упрощенно, функция измеряет каков будет результат, начиная от шага s, если следовать какой-то стратегии применения политики.
Функции ценности применяются так или иначе в любой модели обучения с подкреплением.

Функции ценности используются, чтобы попытаться найти политику, которая максимизирует результат. Для этого выполняется несколько оценок ожидаемого результата при использовании некоторой политики (обычно или текущая политика (on-policy) или оптимальная (off-policy)).

Эти методы основаны на Марковском процессе принятия решений, где оптимальность формулируется как "Политика является оптимальной, если она позволяет достичь наилучший ожидаемый результат из любого начального состояния. Как видно из определения, это и является целью обучения с подкреплением.

Чтобы определить марковский процесс принятия решений, нужно задать 4-кортеж {S -  конечное множество состояний, A - конечное множество действий (часто представляется в виде множеств действий A', доступных из какого-то состояния), P - вероятность, что действие A в состоянии S во время t приведет в состояние S' ко времени t+1}

Обычно применяются 4 основных вида функции:
1. "По политике" (**On-Policy Value Function**), которая рассчитывает ожидаемый результат, если агент будет выполнять действия в строгом соответствии с политикой.
2. "Действие, затем по политике" (**On-Policy Action-Value Function**). Рассчитывает ожидаемый результат, если агент сначала выполнит произвольное действие - возможно, не совпадающее с политикой, затем будет выполнять действия в строгом соответствии с политикой.
3. "Функция оптимальной ценности" (**Optimal Value Function**), которая рассчитывает ожидаемый результат, если агент будет выполнять действия в строгом соответствии с ОПТИМАЛЬНОЙ политикой.
4. "Оптимальное действие, затем функция оптимальной ценности" (**Optimal Action-Value Function**). Рассчитывает ожидаемый результат, если агент сначала выполнит произвольное действие - возможно, не совпадающее с политикой, затем будет выполнять действия в строгом соответствии с ОПТИМАЛЬНОЙ политикой.

Все варианты функции ценности ведут расчет от какого-то момента времени с состоянием s.

##### Уравнение Беллмана / Bellman Equations
Все перечисленные выше 4 функции ценности выполняют уравнение Беллмана. Упрощенно, смысл уравнений Беллмана можно описать так: "Ценность текущей точки является суммой награды в текущей точке плюс ценность следующей точки".

#### Функция примущества / Advantage Functions
Иногда нам нет необходимости точно определить, насколько некоторое действие хорошо в смысле получения результата. Нам достаточно среди доступных действий выбрать лучшее относительно других. То есть, подсчитать ОТНОСИТЕЛЬНОЕ преимущество этого действия.

Функция преимущества A, соответствующая политике Pi, описывает, насколько лучше предпринять некоторое действие A в состоянии S, среди случайно выбранных действивий в соответствии с политикой Pi, предполагая, что далее действия выполняются в соответствии с политикой Pi.

Как видно из определения, функция преимущества позволяет на каждом шаге выбрать отличный от политики шаг. Таким образом, процесс обучения носит менее случайный характер, а время обучения сокращается.

### Попытка классификации алгоритмов обучения с подкреплением

#### Алгоритмы без модели среды алгоритмы и алгоритмы с моделью окружающей среды (Model-Free / Model-Based RL)
Дисклеймер: мы не пытаемся дать исчерпывающее описание всех существующих подходов. Это лишь поверхностный обзор широко известных решений. Кроме того, любая классификация алогоритмов обучения с подкреплением будет весьма условной, так как компоненты одного решения могут быть использованы в другом.

Все семейство алгоритмов можно разделить на 2 класса: безмодельные алгоритмы и алгоритмы с моделью окружающей среды. 

Под моделью окружающей среды здесь мы понимаем математическую модель, способную рассчитать состояние окружающей среды и награду за различные варианты действий.
Наличие модели позволяет агенту планировать свои действия, то есть выбирать оптимальный вариант политики. Наиболее известным реализацией этого варианта является [AlfaZero](https://arxiv.org/abs/1712.01815).

Казалось бы, что этот вариант наиболее предпочтителен, так как позволяет с помощью модели рассчитать оптимальную политику сразу при начале работы. А в дальнейшем ее корректировать.

Проблема в том, что обычно математическая модель окружающей среды недоступна для агента. Ее просто может не быть, как в виду сложности построения такой модели, так и из-за ограничениям по срокам и бюджетам. Даже если математическая модель имеется, реальный мир гораздо сложнее и вариабельнее. Обученный на математической модели агент может показывать неоптимальные результаты в реальном мире, а может и вообще не показывать результатов. Не говоря уж о том, что имплементация этого подхода трудна и требует значительного времени на разработку и вычисления.

Алгоритмы без модели среды (model-free) менее результативны, однако их гораздо легче, быстрее и проще разрабатывать и настраивать. Именно поэтому они (возможно, только пока) наиболее популярны и это направление активно разрабатывается и развивается.

Поскольку для начинающих наиболее актуальна информация о алгоритмах без модели среды, далее в тексте обсуждать алгоритмы с математической моделью мы обсуждать не будем. Здесь перчислим наиболее известные на настоящий момент (лето 2020): MBVE, MBMF, I2A, WorldModels, AlfaGo, AlfaZero.

#### А чему мы учим?
Кажется, ответ на поверхности - учим агента делать правильные действия. Но это если смотреть с точки зрения стороннего наблюдателя. С нашей с вами точки зрения, то есть точки зрения разработчика, можно предложить 3 варианта ответа на этот вопрос:
1. В процессе обучения агент оптимизирует политику (стохастическую или детерминистскую);
2. В процессе обучения агент оптимизирует Q-функцию, т.е функцию действие-ценность (action-value function);
3. В процессе обучения агент оптимизирует функции ценнности

#### Оптимизация политики / Policy Optimization.

В этом подходе политика Pi имеет некоторые входные параметры Тета. Методы этого подхода оптимизируют параметры политики непосредственно с помощью градиентного подъема (gradient ascent), максимизируя эффективность J (performance). Или косвенно, максимизируя локальные аппроксимации J (в случаях, когда функция эффективности J имеет разрывы).

Почти всегда оптимизация выполняется "По политике" (on-policy). Это означает, что оптимизация и обновление политики происходит на основе данных, собранных на последней версии политики. Оптимизация политики также обычно включает обучение апроксиматора функции ценности, который используется для определения способа обновления политики. Функцию ценности на практике невозможно рассчитать точно. Поэтому используют аппроксиматор, который выдает результат, похожий на результат функции ценности, но с некоторой погрешностью. Если считать, что результат аппроксиматора примерно равен функции ценности, то, как указывалось выше, вы можем дать оценку различным вариантам политики.

Несколько примеров алгоритмов оптимизации политики:
* [A2C / A3C](https://arxiv.org/abs/1602.01783). Выполняют градиентный подъем для поиска максимума эффективности (performance);
* [PPO](https://arxiv.org/abs/1707.06347). Этот метод неявным образом максимизирует эффективность. Вместо поиска максимальной эффективности, он максимизирует дополнительную функцию, которая дает оценку, насколько изменится эффективность в результате обновления политики. 
* Q-Learning (Лучше использовать англоязычный термин). Методы этого семейства алгоритмов используют аппроксиматор Q для поиска оптимальной функции действие-ценность Q* (action-value function). Обычно в этих методах используется уравнение Беллмана. В результате работы этого алгоритма выбираются оптимальные действия для текущего шага (off-policy). Это означает, что для обновления политики алгоритм использует собранные во время обучения данные, вне зависимости от того, какие действия агент выбрал в момент сбора данных. Новая политика формируется исходя из значений текущей политики и и полученной на текущем шаге функцией действие-ценность Q*.

Примеры алгоритмов Q-learning:
* [DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), теперь уже классический метод. DQN используется для среды, имеющей дискретной пространство действий. Стандартные методы на момент написания статьи обучали агента каждый раз на новых данных от среды. Авторы предложили обучение проводить в 2 прохода - первый проход НС делает на основе существующей политики, получает от среды награды и/или депремирование. Результаты - состояние среды, действия, награды - записываются в траекторию. После этого проводится оптимизация политики. Для этого авторы изменяют столбец награды в траектории с помощью функции дисконтирования награды. Этот подход авторы назвали повторным получением опыта (experience replay).
* [C51](https://arxiv.org/abs/1707.06887), Categorical DQN - вариант, который учится с помощью распределения результата, ожидания которого есть функция дейстие-ценность Q*.

