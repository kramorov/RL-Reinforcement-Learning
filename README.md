# RL-Reinforcement-Learning
## Обучение с подкреплением
### Целью написания этой статьи является коротко рассказать об обучении с подкреплением, дать описание англоязычным терминам, используемым в RL, в том значении, как они используются в функциональном API TensorFlow.

Главными действующими лицами в RL являются агент (agent) и среда (environment). Среда - это мир, где обитает агент и с которой он взаимодействует. Взаимодействие осуществляется пошагово. На каждом шаге агент видит наблюдение (observation) или состояние (state) среды, и принимает решение, какое действие (action) предпринять. Выполняя наблюдение (observation), агент не всегда может видеть состояние среды или среду целиком. Скажем, в карточной игре, агент не может видеть карты соперника или карты в колоде. Если использоовать эту аналогию, то средой (environment) будут карты на руках у игроков и в колоде, а наблюдением (observation) агента - только карты, которые вскрыты. Продолжая аналогию карточной игры, шаг игры состоит из ходов игроков. После каждого хода меняется состояние (state) среды, и меняется наблюдение игрока. Таким образом, среда, состояние среды может меняться как под воздействием игрока, так и без его участия.

Агент получает сигнал от среды в виде вознаграждения или награды (reward). Русский перевод этого термина не точно воспроизводит смысл этого термина. Reward в терминах RL может быть как наградой (поощрением), так и наказанием (депремированием). В дальнейшем на русском я буду использовать слово "награда". Награда представляет собой некоторое число, которое показывает результат действий агента. Награда выдается средой не на каждом шаге, а только при каком-то изменении среды. Например, в пиг-понге, футболе и т.п. это гол. В карточной игре - завершение очередного розыгрыша. В гонках - столкновение с препятствием или поднятие бонуса (канистра, шина и т.п.).

Целью агента является максимизировать суммарное количество наград за какой-то промежуток времени (сет в теннисе, партия в шашках, продержаться 50 секунд в бою и т.п.). Суммарное количество наград назовем "результат" (return).

Целью обучения с подкреплением является обучение агента такому поведению, чтобы результат был максимальный.

Сначала обсудим термины, используемые в RL:
Состояния  и наблюдения (states and observations);
Пространство действий (action space);
Политики (policies);
Траектории (trajectories);
Различные формулировки результата (different formulations of return);
Оптимизация обучения с подкреплением (RL optimization problem);
Функции (value functions).

### Состояния и наблюдения / States and Observations
Состояние (state)  s является полным описанием среды. Никакая другой информации о среде не имеется. Наблюдение (observation) может не содержать в себе всю информацию о среде, то есть может являться лишь частичным описанием среды.

В глубоком обучении с подкреплением (deep RL) состояния среды или наблюдения являются численным вектром (1D), матрицей (2D), в случае большего количества измерений мы называем это тензором. Например, наблюдение в виде изображения может бють представлено в виде RGB матриц значений пикселов, состояние робота может быть описано вектором, состоящим из значений  положений его частей и/или скорости их движения.

Когда агент в наблюдении видит полное состояние среды, это называется "полностью наблюдаемой средой" (fully observed environment). Когда агент в наблюдении видитлишь какую-то часть состояния среды, это называется "частично наблюдаемой средой" (partially observed environment).
