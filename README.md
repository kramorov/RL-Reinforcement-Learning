# RL-Reinforcement-Learning
## Обучение с подкреплением
### Целью написания этой статьи является коротко рассказать об обучении с подкреплением, дать описание англоязычным терминам, используемым в RL, в том значении, как они используются в функциональном API TensorFlow.

Текст написан намеренно упрощенно, так как рассчитан и на тех, кто недостаточно хорошо знаком с математикой и/или Python/TensorFlow.

Главными действующими лицами в RL являются агент (agent) и среда (environment). Среда - это мир, где обитает агент и с которой он взаимодействует. Взаимодействие осуществляется пошагово. На каждом шаге агент видит наблюдение (observation) или состояние (state) среды, и принимает решение, какое действие (action) предпринять. Выполняя наблюдение (observation), агент не всегда может видеть состояние среды или среду целиком. Скажем, в карточной игре, агент не может видеть карты соперника или карты в колоде. Если использоовать эту аналогию, то средой (environment) будут карты на руках у игроков и в колоде, а наблюдением (observation) агента - только карты, которые вскрыты. Продолжая аналогию карточной игры, шаг игры состоит из ходов игроков. После каждого хода меняется состояние (state) среды, и меняется наблюдение игрока. Таким образом, среда, состояние среды может меняться как под воздействием игрока, так и без его участия.

Агент получает сигнал от среды в виде вознаграждения или награды (reward). Русский перевод этого термина не точно воспроизводит смысл этого термина. Reward в терминах RL может быть как наградой (поощрением), так и наказанием (депремированием). В дальнейшем на русском я буду использовать слово "награда". Награда представляет собой некоторое число, которое показывает результат действий агента. Награда выдается средой не на каждом шаге, а только при каком-то изменении среды. Например, в пиг-понге, футболе и т.п. это гол. В карточной игре - завершение очередного розыгрыша. В гонках - столкновение с препятствием или поднятие бонуса (канистра, шина и т.п.).

Целью агента является максимизировать суммарное количество наград за какой-то промежуток времени (сет в теннисе, партия в шашках, продержаться 50 секунд в бою и т.п.). Суммарное количество наград назовем "результат" (return).

Целью обучения с подкреплением является обучение агента такому поведению, чтобы результат был максимальный.

Сначала обсудим термины, используемые в RL:
Состояния  и наблюдения (states and observations);
Пространство действий (action space);
Политики (policies);
Траектории (trajectories);
Различные формулировки результата (different formulations of return);
Оптимизация обучения с подкреплением (RL optimization problem);
Функции (value functions).

### Состояния и наблюдения / States and Observations
Состояние (state)  s является полным описанием среды. Никакая другой информации о среде не имеется. Наблюдение (observation) может не содержать в себе всю информацию о среде, то есть может являться лишь частичным описанием среды.

В глубоком обучении с подкреплением (deep RL) состояния среды или наблюдения являются численным вектром (1D), матрицей (2D), в случае большего количества измерений мы называем это тензором. Например, наблюдение в виде изображения может бють представлено в виде RGB матриц значений пикселов, состояние робота может быть описано вектором, состоящим из значений  положений его частей и/или скорости их движения.

Когда агент в наблюдении видит полное состояние среды, это называется "полностью наблюдаемой средой" (fully observed environment). Когда агент в наблюдении видитлишь какую-то часть состояния среды, это называется "частично наблюдаемой средой" (partially observed environment).

### Пространство действий / Action Space
В различных средах допускаются различные действия агентов. Набор допустимых со стороны агента действий в данной конкретной среде часто называют "пространством действий". 
В некоторых средах, таких как игры Атари, шашки, карточные игры и т.п., пространство действий является дискретным. Проще говоря, представляет собой конечный набор чисел, каждое из которых означает какое-то отдельное действие (вверх, вниз, включить, выключить и так далее.
В других средах пространство действий может быть протяженным (continuous action space). То есть, каждое действие может быть любым числом из какого-то заданного диапазона значений. Пример - органы управления роботом в реальном мире. В RL они, как правило, являются вещественными векторами (real-valued vectors). Программист бы сказал - одномерный массив чисел типа float.
Тип пространства действий напрямую влияет на выбор алгоритмов глубокого обучения с подкреплением. Некоторые семейства алгоритмов могут быть применены без глубокой переделки только к одному типу пространства действий.

### Политики / Policies
Политика (policy) это правило, используемое агентом для принятия решения, какое действие предпринять. С точки зрения агента, это число или тензор из пространства действий.
Политика может быть детерминированной, то есть это число или тензор, значение которых получается путем каких-то вычислений на основании состояния среды. То есть, оно не является случайным.
Политика может быть стохастической. Это по-прежнему число или тензор. Однако значения этого числа или тензора может быть различным в разные моменты времени при одинаковом состоянии среды.
В глубоком обучении с подкреплением используются параметрические политики. То есть, выход политики является результатом вычисления какой-то функции, на вход котороой подается набор параметров, например веса и биас из НС. В процессе обучения мы подбираем значения параметров с помощью некоторого алгоритма оптимизации. 

Политика является механизмом принятия решения агентом, можно сказать, его мозгом. Имеено поэтому в литературе нередко заменяют термин "политика" словом "агент". Например, "политика нацелена на максимизацию результата".

На практике, красивый термин "политика" по сути является нашей нейронной сетью.

#### Детерминированные политики
Простым примером детерменированной политики является модель многослойного перцептрона (multi-layer perceptron (MLP)). 

model = Sequential()
model.add(Dense(10, activation='relu', input_shape=(n_features,)))
model.add(Dense(8, activation='relu', ))
model.add(Dense(1, activation='tanh'))

Если на вход такой сети после подать вектор длиной n_features, на выходе мы получим число в диапазоне от -1 до +1. То есть, пространсто действий для этой политики будет протяженным, диапазон значений пространства действий [-1, +1].
Эту же архитектуру можно использовать для дискретного пространства действий, просто заменив функцию активации в выходном слое на softmax:

model = Sequential()
model.add(Dense(10, activation='relu', input_shape=(n_features,)))
model.add(Dense(8, activation='relu', ))
model.add(Dense(1, activation='softmax'))

Вы этом случае, детерминированная политика, вычисляемая нашей НС будет ориаентирована на дискретное пространство действий, так как результатом вычислений будут 2 значения: 0 или 1. Этими значениями могут быть закодированы какие-то дейтсвия, например, "0" - движение вверх, "1" - движение вниз.

#### Stochastic Policies
Выходом детерминированной политики является число или тензор, обозначающих значение чего-либо. Выходом стохастической политики также является число или тензор, однако он обозначает не величину действия из пространства действий, а ВЕРОЯТНОСТЬ действия.
Для обучения и работы стохастической политики необходимо сделать 2 ключевых вычислений:
- получить действия из политики
- затем вычислить логарифм вероятности действий.

Наиболее часто используемыми в обучении с подкреплением разновидностями политик являются категорийная политика и диагональная Гауссова политика.
Категорийные политики используются при дискретном пространстве действий, а диагональные Гауссовы  - в протяженном просстранстве действий.
С точки зрения теории вероятности, категорийная политика представляет собой [распределение Бернулли](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%91%D0%B5%D1%80%D0%BD%D1%83%D0%BB%D0%BB%D0%B8). Это означает что величина может принимать только два значание - 0 и 1, значение 0 с вероятностью P, а значение 1 - вероятностью (1-P). 
